---
title: "DATA624 Homework 1"
author: "Esteban Aramayo, Coffy Andrews-Guo, LeTicia Cancel, Joseph Connolly, Ian Costello"
date: "5/31/2022"
output:
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r warning=FALSE, error=FALSE, message=FALSE}
library(ggfortify)
library(openxlsx)
library(fpp2)
library(fma)
library(gridExtra)
library(seasonal)
library(ggplot2)
library(patchwork)
library(caret)
library(grid)
```

# Week 1

## HA 2.1 

### Use the help function to explore what the series `gold`, `woolyrnq` and `gas` represent.

The `fpp2` forecast package for "Forecasting: Principles and Practice" (2nd Edition) was loaded to explore three time series data library: `gold`, `woolyrnq`, and `gas`. The RStudio IDE `help` function is a comprehensive built-in system providing the following: 

Data Set     | Description       | Format   | Source
-------------|-------------------|----------|---------
gold         | Daily morning gold prices in US dollars. 1 January 1985 - 31 March 1989. | Time series data |Not Available
woolyrnq     | Quarterly production of woollen yarn in Australia: tonnes. Mar 1965 - Sep 1994 | Time series data | Time Series Data Library
gas          | Australian monthly gas production: 1956-1995 | Time series data | Australian Bureau of Statistics


```{r helpgold}
#help function for each series using question mark "??"
??gold
```

```{r helpwoolyrnq}
#help function for each series using question mark "??"
??woolyrnq
```

```{r helpgas}
#help function for each series using question mark "??"
??gas
```

The `help` function may provide package code examples and hyperlink to additional research documentation. 



### a. Use `autoplot()` to plot each of these in separate plots.

```{r fig.height=15, fig.width=10}
#autoplots of each series
grid.arrange(autoplot(gold),autoplot(woolyrnq),autoplot(gas))
```

### b. What is the frequency of each series? Hint: apply the `frequency()` function.

- `gold` has a frequency of 1, meaning it is annual.
- `Woolyrng` has a frequency of 1, meaning it is quarterly. 
- `gas` has a frequency of 12, meaning it is monthly.

```{r}
#frequency of each series
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```
**Alternative to (b)**

b. What is the frequency of each series? Hint: apply the `frequency ( )` function.

```{r freqgold, echo=FALSE}
paste0("The frequency for the gold time series is: ", frequency(gold), "  - where the data represents an ANNUAL observations.")
```

```{r freqwool, echo=FALSE}
paste0("The frequency for wool production time series is: ", frequency(woolyrnq), " - where the data represents QUARTERLY observations.")
```

```{r freqgas, echo=FALSE}
paste0("The frequency for gas production time series is: ",  frequency(gas), " - where the data represents MONTHLY observations.")
```







### c. Use `which.max()` to spot the outlier in the `gold` series. Which observation was it?
 
The outlier for the `gold` series is observation number 770, with the value of 593.7.

```{r maxgas, include=FALSE}
#calling which.max() function
which.max(gold)
#printing maximum value
gold[which.max(gold)]
```

**Alternative to (c)**
The first maximum value in the `gold` series is located in the `r which.max(gas)` position with a value of `r gas[which.max(gas)]`.



## HA 2.3

Download some monthly Australian retail data from the book website. These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

### a. You can read the data into R with the following script:
```{r}
retaildata <- read.xlsx("https://otexts.com/fpp2/extrafiles/retail.xlsx",startRow = 2)
```

### b. Select one of the time series as follows (but replace the column name with your own chosen column):
```{r myts2}
myts <- ts(retaildata[,"A3349873A"],
  frequency=12, start=c(1982,4))
#myts  #code is silent because it represents an example - see "myts2"  

myts2 <- ts(retaildata[,"A3349791W"],
            frequency=12, start=c(1982,4))
myts2
```

#### Explore your chosen retail time series using the following functions:

`autoplot()`, `ggseasonplot()`, `ggsubseriesplot()`, `gglagplot()`, `ggAcf()`

#### Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

The first plot using autoplot() shows some seasonality for each year telling us that the price spikes at different points. There is also an upward trend showing that the price of gold continues to increase overtime. The length between the min and max for each of the seasonal spike grows longer each year telling us that the price difference between the minimum and maximum price spreads out more over time. 
```{r autoplot-myts2}
#time series qutoplot
autoplot(myts2)
```

The seasonal plot below using ggseasonlot() gives us more clarity on when the seasonality takes place. Each year has a spike in December with the max prices increasing each year. We can take a guess as to why December is the month that spikes since this is when holiday shopping takes place. 

```{r seasonplot-myts2}
#time series seasonal plot
ggseasonplot(myts2)
```

The subseries plot confirms the seasonality shown in the previous plots with December being the seasonal month. This plot creates a better visualization for a couple of details better than the previous plots. The difference between the minimum and maximum over the years is clearer. The minimum has a minor increase in december but the maximum has a dramatic increase when compared to the other months. The mean also has a big increase in December but then levels out during the spring and summer months.

```{r subseriesplot}
#time series sub series plot
ggsubseriesplot(myts2)
```

The lag plot shows strong positive correlation for all months in lag 12.

**Alternative description**
The scatterplots shows the lagged values of the time series. Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of $k$. The relationship is strongly positive at lag 16, reflecting the strong seasonality in the data.

```{r lagplot, fig.width=15}
#time series lag plot
gglagplot(myts2)
```

The ACF plot supports the lag plot by showing a high autocorrelation coefficient for lag 12. There are also no negative correlations which was not as clear in the subseries plot. They are also all significantly different from zero. 

**Additional description**
The `Acf` graph shows the autocorrelation coefficients, `correlogram`, of the retail sales in various categories for different Australian states base on `A3349791W` time series. The dashed blue lines indicate whether the correlations are significantly different from zero. 

```{r ACF}
#time series ACF
ggAcf(myts2)

```

## HA 6.2
The plastics data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

### a. Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?


```{r fig.width=10, fig.height=15}
#autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf()
p1 <- autoplot(plastics)
p2 <- ggseasonplot(plastics)
p3 <- ggsubseriesplot(plastics)
p4 <- gglagplot(plastics)
p5 <- ggAcf(plastics)
grid.arrange(p1,p2,p3,p5,p4, ncol=1)
```

**Additional description**
The time plot automatically shows the monthly sales (in thousands) of product A for a plastics manufacturer for five years and reveals: (1) there is an increasing trend; (2) there is a mild seasonal pattern that increases in size as the level of the series increases; and (3) the sudden drop at the year end/starting year is due to government subsidies for pollution control, such as [deposit-refund systems](https://media.rff.org/archive/files/sharepoint/WorkImages/Download/RFF-DP-11-47.pdf).

The season plot shows a seasonal pattern occurs from February to December for each year (five years) time series. 


### b. Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.

```{r fig.height=5, warning=FALSE}
#multiplicative decomposition
decomp_plastics <- plastics %>%
  decompose(type = "multiplicative")

decomp_plastics %>% autoplot()
```

### c. Do the results support the graphical interpretation from part a?

The "Data" and "Seasonal" sections of the Decomposition shows similar results to the autoplot() chart. There is a steady seasonal trend that has a similar duration for each seasonal cycle. The "Trend" section of the Decomposition also supports the visuals from part A and shows a steady upward trend from years 1-5. 

**Additional/Alternative description**
The multiplicative decomposition plot supports Part (a) interpretation of a seasonal pattern. The seasonal pattern is unchanging, the remainder component has a lot of large values, and has an increasing trend with some missing observations from the beginning and the end of the data set.


### d. Compute and plot the seasonally adjusted data.

```{r warning=FALSE}
autoplot(plastics, series = "Data") +  
  autolayer(trendcycle(decomp_plastics), series = "Trend") +
  autolayer(seasadj(decomp_plastics), series = "Seasonally Adjusted")

#fit <- seas(x = plastics, x11="")

plastics %>%
  stl(s.window = "periodic", robust = TRUE) %>%
  autoplot()
```

**Alternative/Additional Plot**
```{r}
plastics %>% decompose(type = "multiplicative") -> fit
autoplot(plastics, series = "Data") + 
  autolayer(seasadj(fit), series = "Seasonally Adjusted") +
  xlab("Recorded Years") +
  ggtitle("Multiplicative Decomposition of Plastics Manufacturer Sales Data")
```





### e. Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?
The outlier was applied to the first observation in the plastics data so the chart changed only in year 1. The outlier being in the beginning of the data had no impact on the rest of the plot.
```{r warning=FALSE}
#creating a copy of plastics and adding 500 to one observation
plastics2 <- plastics
plastics2[1] <- plastics2[1] + 500

decomp_plastics2 <- plastics2 %>%
  decompose(type = "multiplicative")

autoplot(plastics2, series = "Data") +  
  autolayer(trendcycle(decomp_plastics2), series = "Trend") +
  autolayer(seasadj(decomp_plastics2), series = "Seasonally Adjusted")
```


**Additional/Alternative Plots**

```{r warning=FALSE}
plasticsOutlier <- plastics
plasticsOutlier[30] <- plasticsOutlier[30] + 500

autoplot(plasticsOutlier, series = "Data") +
  autolayer(trendcycle(fit), series = "Trend") +
  autolayer(seasadj(fit), series = "Seasonally Adjusted") + 
  xlab("Year") + ylab("Monthly Sales (in thousands)") +
  ggtitle("Seasonally Adjusted Plastics Manufacturer Sales Data") +
  scale_color_manual(values = c("blue", "green", "red"),
                     breaks = c("Data", "Trend", "Seasonally Adjusted"))
```

```{r}
plasticsOutlier %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Recorded Years") +
  ggtitle("Multiplicative Decomposition of Plastics Manufacturer Sales Data")
```

The effects of the outlier is noticeable and still supports Part (a) interpretation of a seasonal pattern. The difference is shown in the data component with the changed observation creates a high peak, then change direction and continues along a seasonal pattern.

### f. Does it make any difference if the outlier is near the end rather than in the middle of the time series?

The middle outlier has a greater impact on the Seasonally adjusted line than the end outlier when compared to the original plot. When the last observation is the outlier the Data and Seasonally Adjusted lines trend upward and all other areas on the chart are not impacted. When a middle observation is the outlier we see a different seasonally adusted line when compared to the original chart. The downward and upward spikes around the outlier are sharper on the middle outlier chart while the original chart has subtle changes with a fairly even upward trend. 
```{r fig.height=10, warning=FALSE}
plastics_mid <- plastics
plastics_end <- plastics  
plastics_mid[30] <- plastics_mid[30] + 500 #outlier in the middle
plastics_end[52] <- plastics_end[52] + 500 #outlier to reflect "near the end"

decomp_mid <- plastics_mid %>%
  decompose(type = "multiplicative")

p <- autoplot(plastics, series = "Data") +  
  autolayer(trendcycle(decomp_plastics), series = "Trend") +
  autolayer(seasadj(decomp_plastics), series = "Seasonally Adjusted") +
  ggtitle("Plastics Seasonally Adjusted - Original")

p1 <- autoplot(plastics_mid, series = "Data") +  
  autolayer(trendcycle(decomp_mid), series = "Trend") +
  autolayer(seasadj(decomp_mid), series = "Seasonally Adjusted") +
  ggtitle("Plastics Seasonally Adjusted - Mid Observation")

decomp_end <- plastics_end %>%
  decompose(type = "multiplicative")

p2 <- autoplot(plastics_end, series = "Data") +  
  autolayer(trendcycle(decomp_end), series = "Trend") +
  autolayer(seasadj(decomp_end), series = "Seasonally Adjusted") +
  ggtitle("Plastics Seasonally Adjusted - Near the End Observation")

grid.arrange(p, p1,p2)
```

# Week 2

## KJ 3.1

The UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe. The data can be accessed via:
```{r warning=FALSE, error=FALSE}
library(mlbench)
data(Glass)
str(Glass)
```

### a. Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.
When we explore the Glass dataset using summary we see that there are no NA values for any of the predictors. There are also no negative values with the minimum value being 0.00 (zero) for some predictors. 
```{r}
summary(Glass)
```

```{r fig.width=10, fig.height=10}
ri_b <- ggplot(Glass, aes(x = Type, y = RI)) +
  geom_boxplot()

na_b <- ggplot(Glass, aes(x = Type, y = Na)) +
  geom_boxplot()

mg_b <- ggplot(Glass, aes(x = Type, y = Mg)) +
  geom_boxplot()

al_b <- ggplot(Glass, aes(x = Type, y = Al)) +
  geom_boxplot()

si_b <- ggplot(Glass, aes(x = Type, y = Si)) +
  geom_boxplot()

k_b <- ggplot(Glass, aes(x = Type, y = K)) +
  geom_boxplot()

ca_b <- ggplot(Glass, aes(x = Type, y = Ca)) +
  geom_boxplot()

ba_b <- ggplot(Glass, aes(x = Type, y = Ba)) +
  geom_boxplot()

fe_b <- ggplot(Glass, aes(x = Type, y = Fe)) +
  geom_boxplot()


ri_b+na_b+mg_b+al_b+si_b+k_b+ca_b+ba_b+fe_b+
  plot_layout(ncol=3)
```

```{r fig.height=10, fig.width=10}
ri_d <- ggplot(Glass, aes(RI, color=Type)) + geom_density()
na_d <- ggplot(Glass, aes(Na, color=Type)) + geom_density()
mg_d <- ggplot(Glass, aes(Mg, color=Type)) + geom_density()
al_d <- ggplot(Glass, aes(Al, color=Type)) + geom_density()
si_d <- ggplot(Glass, aes(Si, color=Type)) + geom_density()
k_d <- ggplot(Glass, aes(K, color=Type)) + geom_density()
ca_d <- ggplot(Glass, aes(Ca, color=Type)) + geom_density()
ba_d <- ggplot(Glass, aes(Ba, color=Type)) + geom_density()
fe_d <- ggplot(Glass, aes(Fe, color=Type)) + geom_density()

ri_d+na_d+mg_d+al_d+si_d+k_d+ca_d+ba_d+fe_d+
  plot_layout(ncol=3)
```

```{r}
corrplot::corrplot(cor(Glass[c("RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe")]), method = 'number', type = 'lower')
```

### b. Do there appear to be any outliers in the data? Are any predictors skewed?

The boxplots help us see the outliers for each type for each predictor. Type 2 has outliers for the Ri, Mg, and Ca predictors. The Ba predictor has more data for Type 7 with the rest of the types being scattered. The mean for Si is consistent between all types. When we explore the distribution plots we see that Si is closest to a normal distribution. K, Ba, and Fa are left skewed and Mg is right skewed. The other predictors are slightly skewed.

### c. Are there any relevant transformations of one or more predictors that might improve the classification model?

Experimenting with BoxCox transformations for Ba and Fe since both predictors are heavily skewed to the left
```{r}

```

## KJ 3.2

The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes. The data can be loaded via:
```{r fig.height=10, fig.width=10}
library(mlbench)
data(Soybean)
head(Soybean)
summary(Soybean)
## See ?Soybean for details
```

### a. Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?

All but one predictor contain NA values and can be evaluated to see if it is a degenerate distribution. There are no predictors where NA outnumber real values but there are some with a large number of NA. 'hail', 'sever', 'seed.tmt', and 'germ' have a large amount of NA values so we will explore how that impacts the data later. 

```{r fig.height=15, fig.width=10}
col_names <- colnames(Soybean[-1])

plot_list <- list()

for (i in col_names){
  plot <- ggplot(Soybean, aes_string(Soybean[,i])) +
    geom_bar() +
    xlab(colnames(Soybean[i])) 
  plot_list[[i]] <- plot
    #print(plot)
}

grid.arrange(grobs=plot_list, ncol=4)
```

### b. Roughly 18 % of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

Of the 18% of missing data, 8% come from 19 out of 36 columns. 
```{r fig.width=20,fig.height=10, message=FALSE}
#library(mice)
#library(VIM)
#md.pattern(Soybean)

aggr(Soybean, col=c('navyblue','yellow'),
     numbers=TRUE, sortVars=TRUE,
     labels=names(Soybean), cex.axis=.7,
     gap=3, ylab=c("Missing data","Pattern"))
```


### c. Develop a strategy for handling missing data, either by eliminating predictors or imputation.

I chose to impute the data instead of eliminating predictors altogether. Predictors with NA values still had enough data where I didn't feel that complete removal was necessary. Below is a comparison of the summary for the original dataset and the imputed dataset. After the imputation, the numbers do not differ too much but we would need to continue this analysis to see how much the imputation impacted the dataset. 
```{r}
impute_soybean <- parlmice(Soybean, maxit = 5, m = 1, printFlag = FALSE, seed = 500, cluster.seed = 500)
Soybean_2 <- complete(impute_soybean,1)
```

```{r}
#original Soybean data
summary(Soybean)
#imputed soybean data
summary(Soybean_2)
```
```{r fig.height=15, fig.width=10}
col_names <- colnames(Soybean_2[-1])

plot_list <- list()

for (i in col_names){
  plot <- ggplot(Soybean_2, aes_string(Soybean_2[,i])) +
    geom_bar() +
    xlab(colnames(Soybean_2[i])) 
  plot_list[[i]] <- plot
    #print(plot)
}

grid.arrange(grobs=plot_list, ncol=4)
```


## KJ 635

## HA 7.1 - Exponential Smoothing

### Consider the pigs series — the number of pigs slaughtered in Victoria each month.

View the data set
```{r pigsdataset}
pigs
```


```{r}
help(pigs)
pigs %>% autoplot()
```

**Alternative Description**
The Simple Exponential Method (“SES”) method is suitable to forecast time series with no clear trend or seasonality. The Pigs data does not have a clear pattern and therefore we will use SES.


### a. Use the `ses()` function in R to find the optimal values of \(\alpha\) and \(l_0\) and generate forecasts for the next four months.

Using the `summary()` function $\alpha = 0.2971$ and $l_0 = 77260.0561$.

```{r}
#estimate parameter - the next four months
fc<- ses(pigs, h=4)

#timeseries plot with autoplot() function
fc %>%
  autoplot()
```

```{r}
summary(fc)
```

#### b. Compute a 95% prediction interval for the first forecast using \(\hat{y}\pm 1.96s\) where \(s\) is the standard deviation of the residuals. Compare your interval with the interval produced by R.

From chapter 3.5. Prediction intervals are calculated as $\hat{y}_T+h|T \pm c \hat{\sigma}_h$. The multiplier for 95% interval is 1.96 and the residuals are also equal to the RMSE. Using the SES model formula from part a, the $\hat{y} values are stored in a vector as are the residuals. Then, the values are subbed in for the formula. 

The computed intervals vs. the predicted intervals are quite close. The September 1995 low interval at 95% is 78611.97, compared to the computed 78679.97 only a difference of 68. The high values for the predicted interval at 95% is 119020.80, compared to 118952.84 again differing by 68.

```{r}
y_hat <- c(1.96, -1.96)
s <- sd(residuals(fc))

ses(pigs, h=4)$mean[1]+(y_hat*s)
```

## HA 7.2 

#### Write your own function to implement simple exponential smoothing. The function should take arguments y (the time series), alpha (the smoothing parameter α) and level (the initial level ℓ0). It should return the forecast of the next observation in the series. Does it give the same forecast as ses()?

```{r}

```


## HA 7.3
#### Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the optim() function to find the optimal values of α and ℓ0. Do you get the same values as the ses() function?

```{r}

```


## HA 8.1
#### Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

#### a. Explain the differences among these figures. Do they all indicate that the data are white noise?

#### b. Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

## HA 8.2
#### A classic example of a non-stationary series is the daily closing IBM stock price series (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

## HA 8.6
#### Use R to simulate and plot some data from simple ARIMA models.

#### a. Use the following R code to generate data from an AR(1) model with ϕ1=0.6 and σ2=1. The process starts with y1 = 0.

```{r}
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
  y[i] <- 0.6*y[i-1] + e[i]
```

#### b. Produce a time plot for the series. How does the plot change as you change ϕ1?

#### c. Write your own code to generate data from an MA(1) model with θ1=0.6  and σ2=1.

#### d. Produce a time plot for the series. How does the plot change as you change θ1 ?

#### e. Generate data from an ARMA(1,1) model with  


#### f. Generate data from an AR(2) model with  (Note that these parameters will give a non-stationary series.)

#### g. Graph the latter two series and compare them.

## HA 8.8


