knitr::opts_chunk$set(echo=TRUE, warning=FALSE,
message=FALSE,
collapse = FALSE,
comment = "#>" )
library(elasticnet)
library(caret)
library(MASS)
library(lars)
library(stats)
library(pls)
library(tidyverse)
library(dplyr)
library(RANN)
library(GGally)
library(naniar)
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
df_raw <- ChemicalManufacturingProcess
summary(df_raw)
paste0("Missing Values: ",sum(is.na(ChemicalManufacturingProcess)))
gg_miss_var(df_raw)
preProcess_NAdata_model <- preProcess(df_raw, method ="medianImpute")
df <- predict(preProcess_NAdata_model, newdata = df_raw)
paste0(sum(is.na(df))," values missing after imputation")
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df$Yield, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- df[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- df[-trainRowNumbers,]
preProcValues <- preProcess(trainData, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, trainData)
testTransformed <- predict(preProcValues, testData)
trainTransformed
xTrain <- trainTransformed %>% dplyr::select(-Yield)
yTrain <- trainTransformed %>% dplyr::select(Yield)
lm_model <- train(xTrain, yTrain$Yield, method="lm", trControl=trainControl(method="repeatedcv",repeats=5) )
lm_model
summary(lm_model)
plot(lm_model$finalModel)
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
library(randomForest)
install.packages("randomForest")
library(randomForest)
library(caret)
model1 <- randomForest(y ~ ., data = simulated,
+ importance = TRUE,
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~ ., data = simulated,
+ importance = TRUE,
?randomForest
head(simulated)
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~ ., data = simulated,
+ importance = TRUE,
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~., data = simulated)
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~., data = simulated, importance = TRUE)
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
#library(randomForest)
#library(caret)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
#?randomForest
rfImp1
?varImp
rfImp1
View(rfImp1)
ggplot(rfImp1, aes(Overall)) +
geom_point()
ggplot(rfImp1, aes(x = Overall)) +
geom_point()
dim(rfImp1)
summary(rfImp1)
rfImp1$names <- rownames(rfImp1)
rfImp1
#library(randomForest)
#library(caret)
library(tibble)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1
rfImp1 <- tibble::rownames_to_column(rfImp1, "names")
rfImp1
ggplot(rfImp1, aes(x = names, y = Overall)) +
geom_point()
rfImp1 <- tibble::rownames_to_column(rfImp1, "Predictors")
ggplot(rfImp1, aes(x = names, y = Overall)) +
geom_point()
ggplot(rfImp1, aes(x = names, y = Overall)) +
geom_point(color = "orange", size = 4) +
coord_flip()
ggplot(rfImp1, aes(x = names, y = Overall)) +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
ggplot(rfImp1, aes(x = Predictors, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=0, yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
ggplot(rfImp1, aes(x = Predictors, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=-2, yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
ggplot(rfImp1, aes(x = Predictors, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=-1, yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
ggplot(rfImp1, aes(x = Predictors, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(rfImp1, aes(x = Predictors, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(rfImp1, aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
theme_light() +
coord_flip()
rfImp1 %>%
mutate( = fct_reorder(Predictors, Overall)) %>%
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
xlab("Predictor") +
theme_light() +
coord_flip()
rfImp1 %>%
mutate(name = fct_reorder(Overall, Predictors)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
xlab("Predictor") +
theme_light() +
coord_flip()
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
xlab("Predictor") +
theme_light() +
coord_flip()
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
knitr::opts_chunk$set(echo=TRUE, warning=FALSE,
message=FALSE,
collapse = FALSE,
comment = "#>" )
library(elasticnet)
library(caret)
library(MASS)
library(lars)
library(stats)
library(pls)
library(tidyverse)
library(dplyr)
library(RANN)
library(GGally)
library(naniar)
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
df_raw <- ChemicalManufacturingProcess
summary(df_raw)
paste0("Missing Values: ",sum(is.na(ChemicalManufacturingProcess)))
gg_miss_var(df_raw)
preProcess_NAdata_model <- preProcess(df_raw, method ="medianImpute")
df <- predict(preProcess_NAdata_model, newdata = df_raw)
paste0(sum(is.na(df))," values missing after imputation")
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df$Yield, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- df[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- df[-trainRowNumbers,]
preProcValues <- preProcess(trainData, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, trainData)
testTransformed <- predict(preProcValues, testData)
trainTransformed
xTrain <- trainTransformed %>% dplyr::select(-Yield)
yTrain <- trainTransformed %>% dplyr::select(Yield)
lm_model <- train(xTrain, yTrain$Yield, method="lm", trControl=trainControl(method="repeatedcv",repeats=5) )
lm_model
summary(lm_model)
plot(lm_model$finalModel)
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
head(simulated)
#library(randomForest)
#library(caret)
library(tibble)
model1 <- randomForest(y ~., data = simulated, importance = TRUE, ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1 <- tibble::rownames_to_column(rfImp1, "Predictors")
rfImp1 %>%
mutate(name = fct_reorder(Predictors, Overall)) %>%
ggplot(aes(x = name, y = Overall)) +
geom_segment(aes(x=Predictors, xend=Predictors, y=min(Overall), yend= Overall), color = "grey") +
geom_point(color = "orange", size = 4) +
xlab("Predictor") +
theme_light() +
coord_flip()
