---
title: "Data624 - Project 1"
author: "Esteban Aramayo, Coffy Andrews-Guo, LeTicia Cancel, Joseph Connolly, Ian Costello"
date: '2022-06-21'
output: 
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
# install from CRAN
#install.packages("officedown")

# or GitHub
#remotes::install_github("davidgohel/officedown")
```




Required Libraries
```{r warning=FALSE, message=FALSE}
libraries
library(readxl)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(fpp2)
library(caret)
library(RANN)
library(VIM)
```





# Project Summary

De-identified data was provided to conduct a series of six forecasts of different variables of a provided data set. There are two major requirements of this project:

1. This written report
2. The forecasts and error rates

```{r}
df <- read_excel("data.xls")
head(df)
```

## Data Cleaning & Imputation

```{r}
# Factoring category to get a count of the elements within dataset
df$category <- as.factor(df$category)

summary(df)
```

- All groups are equal in length

- Columns 5-7 (Var03, Var04, Var07) all have same amount of missing values. Very close quartile and min/max values, comparable to column 3 (Var01). Column 4 (Var02) has values that are significantly larger 

##### Handling Missing Data

```{r}
paste0(sum(is.na(df))," values missing from original set")
```

- The dilemma is to decide whether or not it is appropriate to perform an analysis via imputing missing values, or to simply delete them. According to the plot below, generated via "VIM::aggr()", 91.81% of the data is fulfilled. Var01, Var02, Var03, Var05, and Var07 are missing about 8% of data. At first impression, this seems like an insignificant amount of data that can be omitted from the set. Further investigation is needed to confirm this impression, and deletion will be deemed appropriate when the data is found to be missing completely at random (MCAR) 

```{r fig.width=10}
# Plots of missing values

aggr_plot <- VIM::aggr(df, col = c("navyblue", "orange"), 
                  numbers = T, sortVars = T,
                  labels = names(df),
                  cex.axis = 0.7, gap = 3,
                  ylab = c("Frequency of Missing Data", "Pattern"))
```

##### Impute or Delete?

- An excerpt from the following paper, $\underline{The\ prevention\ and\ handling\ of\ the\ missing\ data}$, by Hyun Kang, argues when deletion is appropriate or not from the following quote: *"...if the assumption of MCAR (missing completely at random) is satisfied, a listwise deletion is known to produce unbiased estimates and conservative results. When the data do not fulfill the assumption of MCAR, listwise deletion may cause bias in the estimates of the parameters. If there is a large enough sample, where power is not an issue, and the assumption of MCAR is satisfied, the listwise deletion may be a reasonable strategy. However, when there is not a large sample, or the assumption of MCAR is not satisfied, the listwise deletion is not the optimal strategy"* [$^1$](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/)

Creating a shadow matrix to see missing values will indicate whether or not the data is MCAR. 1 indicates all values are present, and anything else indicates the ratio/percentage of missing values correlated among each other [$^2$](https://stats.stackexchange.com/questions/172316/a-statistical-approach-to-determine-if-data-are-missing-at-random)


```{r}
# Shadow Matrix: correlation of missing values from the dataset

x <- as.data.frame(abs(is.na(df))) 

y <- x[which(sapply(x, sd) >0)] # Extracts which variables are missing/NA from the dataset

cor(y) # Tendency of NA when correlated among variables
```

Aside from considering values correlated with themselves, the following have no missing values when correlated with others:

  - Var03 has no missing values when correlated with Var05 and Var07
  
  - Var05 has no missing values when correlated with Var03 and Var07
  
  - Var07 has no missing values when correlated with Var03 and Var05
  
Taking these observations into consideration, it seems there appears to be bias in the data in the context of missing data. Therefore, it seems appropriate to impute the data for missing values since there is evidence they are not missing completely at random.

##### Data Imputation

- Imputation via "medianImpute" method within "preProcess()" function

```{r}
preProcess_NAdata_model <- preProcess(as.data.frame(df), method ="medianImpute")

df <- predict(preProcess_NAdata_model, newdata = df)

paste0(sum(is.na(df))," values missing after imputation")
```

- No data is missing after imputation

```{r}
summary(df)
```



## Data Visualization - Predicators

A time series that contain a list of numbers (the measurements), along with some information about what times those numbers were recorded (the index). 


*Box plots*

- Var02 has the most outliers but this is also the column with the largest range in values
```{r warning=FALSE, message=FALSE, fig.width=6, fig.height=8}
p1 <- ggplot(df, aes(Var01, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p2 <- ggplot(df, aes(Var02, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p3 <- ggplot(df, aes(Var03, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p4 <- ggplot(df, aes(Var05, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p5 <- ggplot(df, aes(Var07, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p6 <- ggplot(df, aes(SeriesInd, fill = category)) + 
  geom_boxplot(outlier.color = "red", outlier.size = 10)

(p1+p2)/(p3+p4)/(p5+p6)
```

All predictors are left skewed so transformations are needed for each column

```{r warning=FALSE, fig.width=10, fig.height=7}
p1 <- ggplot(df, aes(Var01, fill=category)) +
  geom_density(alpha = 0.5)
p2 <- ggplot(df, aes(Var02, fill=category)) +
  geom_density(alpha = 0.5)
p3 <- ggplot(df, aes(Var03, fill=category)) +
  geom_density(alpha = 0.5)
p4 <- ggplot(df, aes(Var05, fill=category)) +
  geom_density(alpha = 0.5)
p5 <- ggplot(df, aes(Var07, fill=category)) +
  geom_density(alpha = 0.5)

p1+p2+p3+p4+p5+
  plot_layout(ncol = 2)
```

## Data transformations

Var02 is the most skewed
```{r}
library(moments)
skewness(df$Var01)
skewness(df$Var02)
skewness(df$Var03)
skewness(df$Var05)
skewness(df$Var07)
```

Experimenting with 3 different transformations; log, square root, and cube root. The log transformation looks the most normalized

```{r error=FALSE}
log_var01 <- log10(df$Var01)
sqrt_var01 <- sqrt(df$Var01)
cube_var01 <- df$Var01^(1/3)

hist(df$Var01)
hist(log_var01)
hist(sqrt_var01)
hist(cube_var01)


```

Transform all predictors using log transformation

```{r}
df_transformed <- df
df_transformed$Var01 <- log10(df$Var01)
df_transformed$Var02 <- log10(df$Var02)
df_transformed$Var03 <- log10(df$Var03)
df_transformed$Var05 <- log10(df$Var05)
df_transformed$Var07 <- log10(df$Var07)
```

New plots using transformed dataframe. Var02 is the most normalize so I will try different transformations with the other columns

```{r warning=FALSE, fig.width=10, fig.height=7}
p1 <- ggplot(df_transformed, aes(Var01, color=category)) +
  geom_density()
p2 <- ggplot(df_transformed, aes(Var02, color=category)) +
  geom_density()
p3 <- ggplot(df_transformed, aes(Var03, color=category)) +
  geom_density()
p4 <- ggplot(df_transformed, aes(Var05, color=category)) +
  geom_density()
p5 <- ggplot(df_transformed, aes(Var07, color=category)) +
  geom_density()

p1+p2+p3+p4+p5+
  plot_layout(ncol = 2)
```






