---
title: "Data624 - Project 1"
author: "Esteban Aramayo, Coffy Andrews-Guo, LeTicia Cancel, Joseph Connolly, Ian Costello"
date: '2022-06-21'
output: 
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE,
                      message=FALSE,
                      collapse = FALSE,
                      comment = "#>" )
```

```{r echo=FALSE}
# install from CRAN
#install.packages("officedown")

# or GitHub
#remotes::install_github("davidgohel/officedown")
```

```{r warning=FALSE, message=FALSE}
library(readxl)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(fpp2)
library(caret)
library(RANN)
library(VIM)
library(ggpubr)
library(gridExtra)
library(forecast)
```

# Project Summary

De-identified data was provided to conduct a series of six forecasts of different variables of a provided data set. There are two major requirements of this project:

1. This written report
2. The forecasts and error rates

```{r}
df <- read_excel("data.xls")
```

The data set does not appear to have any distinguishing labels that would indicate anything about the source or purpose of the data set. Under normal circumstances, context about data and use case is important for forecasting. Context may help identify the kinds of methods and models that would be best suited to produce an accurate result - following the "no free lunch" principle. Therefore, this dataset is completely for practice and exercising forecasting.

## Note on Libraries Used

- Standard libraries
- Particularly useful libraries

# Data Exploration

- The provided data set contains 7 columns and 10,572 rows. The first column, "SeriesInd", is a datetime variable which can be converted to reflect a date. "category" is a classification of each row, of which there are 6 different ones, and the remaining 5 columns are variables to forecast and are the key ingredient for constructing timeseries.

- Variables missing 04 and 06... that's a question mark

A time series that contain a list of numbers (the measurements), along with some information about what times those numbers were recorded (the index). 

**Figure 1 Density Plots**

```{r warning=FALSE, fig.width=10, fig.height=7}
# Density Plot

p1 <- ggplot(df, aes(Var01, fill=category)) +
  geom_density(alpha = 0.5)
p2 <- ggplot(df, aes(Var02, fill=category)) +
  geom_density(alpha = 0.5)
p3 <- ggplot(df, aes(Var03, fill=category)) +
  geom_density(alpha = 0.5)
p4 <- ggplot(df, aes(Var05, fill=category)) +
  geom_density(alpha = 0.5)
p5 <- ggplot(df, aes(Var07, fill=category)) +
  geom_density(alpha = 0.5)

p1+p2+p3+p4+p5+
  plot_layout(ncol = 2)
```

**Figure 2 Box Plots**

```{r warning=FALSE, fig.width=10, fig.height=4}
# Boxplots of Predictors

p1bp <- ggplot(df, aes(category, Var01)) +
  geom_boxplot() + theme_classic()
p2bp <- ggplot(df, aes(category, Var02)) +
  geom_boxplot() + theme_classic()
p3bp <- ggplot(df, aes(category, Var03)) +
  geom_boxplot() + theme_classic()
p4bp <- ggplot(df, aes(category, Var05)) +
  geom_boxplot() + theme_classic()
p5bp <- ggplot(df, aes(category, Var07)) +
  geom_boxplot() + theme_classic()

p1bp + p2bp + p3bp + p4bp + p5bp
```

- Var02 has the most outliers. It's also of a much larger a magnitude from the other predictors

##### Datetime conversion

- Datetime conversion is to be performed after imputation in order to ensure a successful imputation

```{r}
# Converting Var02 to Datetime
df$SeriesInd <- as.integer(df$SeriesInd)
df$SeriesInd <- as.POSIXct(df$SeriesInd, origin = "1970-01-01")


# Renaming SeriesInd to Date to clarify purpose

df <- df %>% rename("Datetime" = SeriesInd)
```

**Figure 3 Outliers**

```{r warning=FALSE, message=FALSE, fig.width=6, fig.height=8}
p1 <- ggplot(df, aes(Var01, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p2 <- ggplot(df, aes(Var02, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p3 <- ggplot(df, aes(Var03, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p4 <- ggplot(df, aes(Var05, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p5 <- ggplot(df, aes(Var07, fill = category)) +
  geom_boxplot(outlier.color = "red", outlier.size = 3)
p6 <- ggplot(df, aes(Datetime, fill = category)) + 
  geom_boxplot(outlier.color = "red", outlier.size = 10)

(p1+p2)/(p3+p4)/p5
```

# Data Preparation

##### Handling Missing Data

```{r}
paste0(sum(is.na(df))," values missing from original set")
```

Looking at the summary generated above, columns 3-7 each have a range of 842-866 missing values. The dilemma is to decide whether or not it is appropriate to perform an analysis via imputing missing values, or to simply delete them. According to the plot below, generated via "VIM::aggr()", 91.81% of the data is fulfilled. Var01, Var02, Var03, Var05, and Var07 are missing about 8% of data. At first impression, this seems like an insignificant amount of data that can be omitted from the set. Further investigation is needed to confirm this impression, and deletion will be deemed appropriate if and only the data is found to be missing completely at random (MCAR).

##### Impute or Delete?

An excerpt from the following paper, $\underline{The\ prevention\ and\ handling\ of\ the\ missing\ data}$, by Hyun Kang, argues when deletion is appropriate or not from the following quote: *"...if the assumption of MCAR (missing completely at random) is satisfied, a listwise deletion is known to produce unbiased estimates and conservative results. When the data do not fulfill the assumption of MCAR, listwise deletion may cause bias in the estimates of the parameters. If there is a large enough sample, where power is not an issue, and the assumption of MCAR is satisfied, the listwise deletion may be a reasonable strategy. However, when there is not a large sample, or the assumption of MCAR is not satisfied, the listwise deletion is not the optimal strategy"* [$^1$](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/)

```{r fig.width=10}
# Plots of missing values

aggr_plot <- VIM::aggr(df, col = c("navyblue", "orange"), 
                  numbers = T, sortVars = T,
                  labels = names(df),
                  cex.axis = 0.7, gap = 3,
                  ylab = c("Frequency of Missing Data", "Pattern"))
```


By creating a shadow matrix to see a percentage (on a 0-1 scale) of missing values when all correlated among each other, this will help indicate whether or not the data is MCAR. [$^2$](https://stats.stackexchange.com/questions/172316/a-statistical-approach-to-determine-if-data-are-missing-at-random)

```{r}
# Shadow Matrix: correlation of missing values from the dataset

x <- as.data.frame(abs(is.na(df))) 

y <- x[which(sapply(x, sd) >0)] # Extracts which variables are missing/NA from the dataset

cor(y) # Tendency of NA when correlated among variables
```

Aside from considering values correlated with themselves, the following have no missing values when correlated with others:

  - Var03 has no missing values when correlated with Var05 and Var07
  
  - Var05 has no missing values when correlated with Var03 and Var07
  
  - Var07 has no missing values when correlated with Var03 and Var05
  
Taking these observations into consideration, it seems there appears to be bias in the data in the context of missing values and therefore not missing completely at random. Imputing the values is an appropriate step to take.

##### Data Imputation

As stated, since the missing values are not MCAR, imputation will be utilized to represent missing values in our data set. There are numerous methods to impute data, especially in R. Within the caret package, the "preProcess()" function enables imputation, and allows for users to select a method of imputation. The method "medianImpute" was chosen because of the ease and efficiency.

```{r}
# Imputation via "medianImpute" method within "preProcess()" function via the caret package

preProcess_NAdata_model <- preProcess(as.data.frame(df), method ="medianImpute")

df <- predict(preProcess_NAdata_model, newdata = df)

paste0(sum(is.na(df))," values missing after imputation")
```

- No data are missing after imputation
- No discrepancies in the data after imputation.

```{r}
# For forecasting later on

s01 <- df %>% filter(category == "S01")
s02 <- df %>% filter(category == "S02")
s03 <- df %>% filter(category == "S03")
s04 <- df %>% filter(category == "S04")
s05 <- df %>% filter(category == "S05")
s06 <- df %>% filter(category == "S06")
```

## Data transformations

By using the skewness function within the moments package, we can see which predictor variable is skewed the most in quantifiable terms.

```{r}
library(moments)
paste0("Var01 skewness: ", skewness(df$Var01))
paste0("Var02 skewness: ",skewness(df$Var02))
paste0("Var03 skewness: ",skewness(df$Var03))
paste0("Var05 skewness: ",skewness(df$Var05))
paste0("Var07 skewness: ",skewness(df$Var07))
```

- Var02 is the most skewed

In order to improve the distribution for the predictors, we can investigate with applying 3 different transformations; log, square root, and cube root. Applying these transformations to one predictor will indicate which will provide the most normal distribution

```{r error=FALSE}
log_var01 <- log10(df$Var01)
sqrt_var01 <- sqrt(df$Var01)
cube_var01 <- df$Var01^(1/3)

hist(df$Var01)
hist(log_var01)
hist(sqrt_var01)
hist(cube_var01)
```

It's revealed the log transformation appears to follow the most normal distribution. Therefore, the log transformation will be applied to all predictor variables.

```{r}
df_transformed <- df
df_transformed$Var01 <- log10(df$Var01)
df_transformed$Var02 <- log10(df$Var02)
df_transformed$Var03 <- log10(df$Var03)
df_transformed$Var05 <- log10(df$Var05)
df_transformed$Var07 <- log10(df$Var07)
```

New plots using transformed dataframe. Var02 is the most normalize so we will try different transformations with the other columns

```{r warning=FALSE, fig.width=10, fig.height=7}
p1 <- ggplot(df_transformed, aes(Var01, fill=category)) +
  geom_density()
p2 <- ggplot(df_transformed, aes(Var02, fill=category)) +
  geom_density()
p3 <- ggplot(df_transformed, aes(Var03, fill=category)) +
  geom_density()
p4 <- ggplot(df_transformed, aes(Var05, fill=category)) +
  geom_density()
p5 <- ggplot(df_transformed, aes(Var07, fill=category)) +
  geom_density()

p1+p2+p3+p4+p5+
  plot_layout(ncol = 2)
```

```{r}
s01 <- df_transformed %>% dplyr::filter(category == "S01")
s02 <- df_transformed %>% dplyr::filter(category == "S02")
s03 <- df_transformed %>% dplyr::filter(category == "S03")
s04 <- df_transformed %>% dplyr::filter(category == "S04")
s05 <- df_transformed %>% dplyr::filter(category == "S05")
s06 <- df_transformed %>% dplyr::filter(category == "S06")
```


Final step before forecasting, create time series for each subset using the data frame with dates.

```{r}
# New time conversion (experimental?)

df_test <- read_excel("data.xls")

# Converting Var02 to Datetime
df_test$SeriesInd <- as.Date(df_test$SeriesInd, origin = "1899-12-30")


# Renaming SeriesInd to Date to clarify purpose
df_test <- df_test %>% rename("Date" = SeriesInd)

#new imputation
preProcess_NAdata_model <- preProcess(as.data.frame(df_test), method ="medianImpute")

df_test <- predict(preProcess_NAdata_model, newdata = df_test)

#new subsets with data conversion
s01_2 <- df %>% filter(category == "S01")
s02_2 <- df %>% filter(category == "S02")
s03_2 <- df %>% filter(category == "S03")
s04_2 <- df %>% filter(category == "S04")
s05_2 <- df %>% filter(category == "S05")
s06_2 <- df %>% filter(category == "S06")
```

```{r}
s01_ts <- ts(s01_2[,c("Var01","Var02")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
s02_ts <- ts(s02_2[,c("Var02","Var03")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
s03_ts <- ts(s03_2[,c("Var05","Var07")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
s04_ts <- ts(s04_2[,c("Var01","Var02")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
s05_ts <- ts(s05_2[,c("Var02","Var03")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
s06_ts <- ts(s06_2[,c("Var05","Var07")], frequency = 12, start = c(2011, 5), end = c(2018, 5))
```

# Forecasting

Now that the data has been transformed, we can begin forecasting 

##### Auto and Seasonal Plots for S01: Var01, 02

For Var01, there are a few patterns to be pointed out. Annually, the value follows a slight valley pattern as it steadily decreases from the beginning to gradually rise towards the end. Monthly, there appears to be a pattern; toward the beginning of each month, the value decreases to a significant low only to lead up to a high-point approximately 2.5 points above the bottom, with a slight correction along the way. This timeseries is also the closest to a white noise series. 

- Same approach was applied to all other variables
- Generally similar to variable 1 to illustrate approach
- Only showing variable 1 for brevity

```{r fig.width=15, fig.height=15}
(autoplot(s01_ts, facets = 2)) / 
  (ggseasonplot(s01_ts[,1], main = "Var01") + ggseasonplot(s01_ts[,2], main = "Var02")) / 
  (ggsubseriesplot(s01_ts[,1]) + ggsubseriesplot(s01_ts[,2])) / 
  (ggAcf(s01_ts[,2], main = "") + ggAcf(s01_ts[,2], main = "")) 
```

## Data Modeling and Forecasting

- Explain Models used - Naive method, STL + Random Walk + Holt Winters
- Confidence intervals are narrow with HoltWinters, and variable on the line point forecast
- Show category 1 plots for brevity, but all other categories followed similar approach
- Final selection on Holt Winters because... 

**Forecasting S01: Var01 & Var02 with decomposition**

```{r fig.height=10, fig.width=15}
#STL using default values
fit_stl_1 <- stl(s01_ts[,1], s.window = "periodic")

#STL using default values
fit_stl_2 <- stl(s01_ts[,2], s.window = "periodic")

#forecast of seasonaly adjusted data
f1 <- fit_stl_1 %>% seasadj() %>% naive()%>%
  autoplot() + ylab("S01: Var01")

#forecast of seasonaly adjusted data
f2 <- fit_stl_2 %>% seasadj() %>% naive()%>%
  autoplot() + ylab("S01: Var02")

#forecast from SLT + Random walk
f3 <- fit_stl_1 %>% forecast(method="naive") %>%
  autoplot() + ylab("S01: Var01")

#forecast from SLT + Random walk
f4 <- fit_stl_2 %>% forecast(method="naive") %>%
  autoplot() + ylab("S01: Var02")

#forecast from Holt-Winters
hw1 <- HoltWinters(s01_ts[,1])

#forecast from Holt-Winters
hw2 <- HoltWinters(s01_ts[,2])

f5 <- hw1 %>% forecast() %>%
  autoplot() + ylab("S01: Var01")

f6 <- hw2 %>% forecast() %>%
  autoplot() + ylab("S01: Var02")

(f1 + f2) / (f3 + f4) / (f5 + f6)
```

## Other Forecast Models

- SES is... 7.1 for broad defintion
- Point forecast is straightline

- ETS 

- ARIMA

Category     | Variable  | ARIMA autoplot() Selection
-------------|-----------|---------------------------
s01          |Var01      | ARIMA(0,1,0)
s01          |Var02      | ARIMA(1,0,0) w/ non-zero mean
s02          |Var02      | ARIMA(0,0,0) w/ non-zero mean
s02          |Var03      | ARIMA(0,1,2)
s03          |Var05      | ARIMA(0,1,0)
s03          |Var07      | ARIMA(0,1,0)
s04          |Var01      | ARIMA(0,1,0)
s04          |Var02      | ARIMA(1,0,0)(0,0,1)[12] w/ non-zero mean
s05          |Var02      | ARIMA(2,0,0) w/ non-zero mean
s05          |Var03      | ARIMA(0,1,2)
s06          |Var05      | ARIMA(0,1,1)
s06          |Var07      | ARIMA(0,1,1)








